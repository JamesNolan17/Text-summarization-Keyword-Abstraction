{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27106449",
   "metadata": {},
   "source": [
    "# Keyword Extraction with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba986a",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e5e6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "167569e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of \n",
    "         learning a function that maps an input to an output based \n",
    "         on example input-output pairs.[1] It infers a function \n",
    "         from labeled training data consisting of a set of \n",
    "         training examples.[2] In supervised learning, each \n",
    "         example is a pair consisting of an input object \n",
    "         (typically a vector) and a desired output value (also \n",
    "         called the supervisory signal). A supervised learning \n",
    "         algorithm analyzes the training data and produces an \n",
    "         inferred function, which can be used for mapping new \n",
    "         examples. An optimal scenario will allow for the algorithm \n",
    "         to correctly determine the class labels for unseen \n",
    "         instances. This requires the learning algorithm to  \n",
    "         generalize from the training data to unseen situations \n",
    "         in a 'reasonable' way (see inductive bias).\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb17a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_gram_range = (1, 1) # (1,1): keyword, (3,3): each keyprase has 3 words\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c850f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97bacf3",
   "metadata": {},
   "source": [
    "## cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d261e489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mapping', 'class', 'training', 'algorithm', 'learning']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "n = 5 # number of keywords\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-n:]]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88ba38",
   "metadata": {},
   "source": [
    "## max sum similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ad0092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bfe146f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maps', 'input', 'class', 'training', 'algorithm']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords2 = max_sum_sim(doc_embedding, candidate_embeddings, candidates, 5,10)\n",
    "keywords2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b37091",
   "metadata": {},
   "source": [
    "## maximal marginal relevance\n",
    "EmbedRank has implemented a version of MMR that allows us to use it for diversifying our keywords/keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a8f89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mmr(doc_embedding, word_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9628e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning', 'algorithm', 'training', 'class', 'mapping']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords3 = mmr(doc_embedding, candidate_embeddings, candidates, 5,0.2)\n",
    "keywords3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7da262",
   "metadata": {},
   "source": [
    "# TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7a1cd9",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e5a41ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"Compatibility of systems of linear constraints over the set of natural numbers. \\\n",
    "Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and \\\n",
    "nonstrict inequations are considered. \\\n",
    "Upper bounds for components of a minimal set of solutions and \\\n",
    "algorithms of construction of minimal generating sets of solutions for all \\\n",
    "types of systems are given. \\\n",
    "These criteria and the corresponding algorithms for constructing \\\n",
    "a minimal supporting set of solutions can be used in solving all the \\\n",
    "considered types of systems and systems of mixed types.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b7bf9",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c393e310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: \n",
      "\n",
      "['compatibility', 'of', 'systems', 'of', 'linear', 'constraints', 'over', 'the', 'set', 'of', 'natural', 'numbers', '.', 'criteria', 'of', 'compatibility', 'of', 'a', 'system', 'of', 'linear', 'diophantine', 'equations', ',', 'strict', 'inequations', ',', 'and', 'nonstrict', 'inequations', 'are', 'considered', '.', 'upper', 'bounds', 'for', 'components', 'of', 'a', 'minimal', 'set', 'of', 'solutions', 'and', 'algorithms', 'of', 'construction', 'of', 'minimal', 'generating', 'sets', 'of', 'solutions', 'for', 'all', 'types', 'of', 'systems', 'are', 'given', '.', 'these', 'criteria', 'and', 'the', 'corresponding', 'algorithms', 'for', 'constructing', 'a', 'minimal', 'supporting', 'set', 'of', 'solutions', 'can', 'be', 'used', 'in', 'solving', 'all', 'the', 'considered', 'types', 'of', 'systems', 'and', 'systems', 'of', 'mixed', 'types', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# clean text data\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    printable = set(string.printable)\n",
    "    text = filter(lambda x: x in printable, text)\n",
    "    text = \"\".join(list(text))\n",
    "    return text\n",
    "\n",
    "Cleaned_text = clean(Text)\n",
    "# print(Cleaned_text)\n",
    "text = word_tokenize(Cleaned_text)\n",
    "\n",
    "print (\"Tokenized Text: \\n\")\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67b0a563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text with POS tags: \n",
      "\n",
      "[('compatibility', 'NN'), ('of', 'IN'), ('systems', 'NNS'), ('of', 'IN'), ('linear', 'JJ'), ('constraints', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('set', 'NN'), ('of', 'IN'), ('natural', 'JJ'), ('numbers', 'NNS'), ('.', '.'), ('criteria', 'NNS'), ('of', 'IN'), ('compatibility', 'NN'), ('of', 'IN'), ('a', 'DT'), ('system', 'NN'), ('of', 'IN'), ('linear', 'JJ'), ('diophantine', 'NN'), ('equations', 'NNS'), (',', ','), ('strict', 'JJ'), ('inequations', 'NNS'), (',', ','), ('and', 'CC'), ('nonstrict', 'JJ'), ('inequations', 'NNS'), ('are', 'VBP'), ('considered', 'VBN'), ('.', '.'), ('upper', 'JJ'), ('bounds', 'NNS'), ('for', 'IN'), ('components', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('minimal', 'JJ'), ('set', 'NN'), ('of', 'IN'), ('solutions', 'NNS'), ('and', 'CC'), ('algorithms', 'NN'), ('of', 'IN'), ('construction', 'NN'), ('of', 'IN'), ('minimal', 'JJ'), ('generating', 'VBG'), ('sets', 'NNS'), ('of', 'IN'), ('solutions', 'NNS'), ('for', 'IN'), ('all', 'DT'), ('types', 'NNS'), ('of', 'IN'), ('systems', 'NNS'), ('are', 'VBP'), ('given', 'VBN'), ('.', '.'), ('these', 'DT'), ('criteria', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('corresponding', 'JJ'), ('algorithms', 'NN'), ('for', 'IN'), ('constructing', 'VBG'), ('a', 'DT'), ('minimal', 'JJ'), ('supporting', 'NN'), ('set', 'NN'), ('of', 'IN'), ('solutions', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('in', 'IN'), ('solving', 'VBG'), ('all', 'PDT'), ('the', 'DT'), ('considered', 'VBN'), ('types', 'NNS'), ('of', 'IN'), ('systems', 'NNS'), ('and', 'CC'), ('systems', 'NNS'), ('of', 'IN'), ('mixed', 'JJ'), ('types', 'NNS'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging for Lemmatization\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "  \n",
    "POS_tag = nltk.pos_tag(text)\n",
    "\n",
    "print (\"Tokenized Text with POS tags: \\n\")\n",
    "print (POS_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3de950d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text tokens after lemmatization of adjectives and nouns: \n",
      "\n",
      "['compatibility', 'of', 'system', 'of', 'linear', 'constraint', 'over', 'the', 'set', 'of', 'natural', 'number', '.', 'criterion', 'of', 'compatibility', 'of', 'a', 'system', 'of', 'linear', 'diophantine', 'equation', ',', 'strict', 'inequations', ',', 'and', 'nonstrict', 'inequations', 'are', 'considered', '.', 'upper', 'bound', 'for', 'component', 'of', 'a', 'minimal', 'set', 'of', 'solution', 'and', 'algorithm', 'of', 'construction', 'of', 'minimal', 'generating', 'set', 'of', 'solution', 'for', 'all', 'type', 'of', 'system', 'are', 'given', '.', 'these', 'criterion', 'and', 'the', 'corresponding', 'algorithm', 'for', 'constructing', 'a', 'minimal', 'supporting', 'set', 'of', 'solution', 'can', 'be', 'used', 'in', 'solving', 'all', 'the', 'considered', 'type', 'of', 'system', 'and', 'system', 'of', 'mixed', 'type', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "adjective_tags = ['JJ','JJR','JJS']  # only want adjectives and nouns\n",
    "\n",
    "lemmatized_text = []\n",
    "\n",
    "for word in POS_tag:\n",
    "    if word[1] in adjective_tags:\n",
    "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "    else:\n",
    "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
    "        \n",
    "print (\"Text tokens after lemmatization of adjectives and nouns: \\n\")\n",
    "print (lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55584023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized text with POS tags: \n",
      "\n",
      "[('compatibility', 'NN'), ('of', 'IN'), ('system', 'NN'), ('of', 'IN'), ('linear', 'JJ'), ('constraint', 'NN'), ('over', 'IN'), ('the', 'DT'), ('set', 'NN'), ('of', 'IN'), ('natural', 'JJ'), ('number', 'NN'), ('.', '.'), ('criterion', 'NN'), ('of', 'IN'), ('compatibility', 'NN'), ('of', 'IN'), ('a', 'DT'), ('system', 'NN'), ('of', 'IN'), ('linear', 'JJ'), ('diophantine', 'JJ'), ('equation', 'NN'), (',', ','), ('strict', 'JJ'), ('inequations', 'NNS'), (',', ','), ('and', 'CC'), ('nonstrict', 'JJ'), ('inequations', 'NNS'), ('are', 'VBP'), ('considered', 'VBN'), ('.', '.'), ('upper', 'JJ'), ('bound', 'NN'), ('for', 'IN'), ('component', 'NN'), ('of', 'IN'), ('a', 'DT'), ('minimal', 'JJ'), ('set', 'NN'), ('of', 'IN'), ('solution', 'NN'), ('and', 'CC'), ('algorithm', 'NN'), ('of', 'IN'), ('construction', 'NN'), ('of', 'IN'), ('minimal', 'JJ'), ('generating', 'VBG'), ('set', 'NN'), ('of', 'IN'), ('solution', 'NN'), ('for', 'IN'), ('all', 'DT'), ('type', 'NN'), ('of', 'IN'), ('system', 'NN'), ('are', 'VBP'), ('given', 'VBN'), ('.', '.'), ('these', 'DT'), ('criterion', 'NN'), ('and', 'CC'), ('the', 'DT'), ('corresponding', 'JJ'), ('algorithm', 'NN'), ('for', 'IN'), ('constructing', 'VBG'), ('a', 'DT'), ('minimal', 'JJ'), ('supporting', 'NN'), ('set', 'NN'), ('of', 'IN'), ('solution', 'NN'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('in', 'IN'), ('solving', 'VBG'), ('all', 'PDT'), ('the', 'DT'), ('considered', 'VBN'), ('type', 'NN'), ('of', 'IN'), ('system', 'NN'), ('and', 'CC'), ('system', 'NN'), ('of', 'IN'), ('mixed', 'JJ'), ('type', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Add POS tag again\n",
    "POS_tag = nltk.pos_tag(lemmatized_text)\n",
    "\n",
    "print (\"Lemmatized text with POS tags: \\n\")\n",
    "print (POS_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67e85e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add punctuations to stopwords\n",
    "stopwords = []\n",
    "\n",
    "wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
    "\n",
    "for word in POS_tag:\n",
    "    if word[1] not in wanted_POS:\n",
    "        stopwords.append(word[0])\n",
    "\n",
    "punctuations = list(str(string.punctuation))\n",
    "\n",
    "stopwords = stopwords + punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ba178e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in more stopwords\n",
    "stopword_file = open(\"stopwords.txt\", \"r\")\n",
    "#Source = https://www.ranks.nl/stopwords\n",
    "\n",
    "lots_of_stopwords = []\n",
    "\n",
    "for line in stopword_file.readlines():\n",
    "    lots_of_stopwords.append(str(line.strip()))\n",
    "\n",
    "stopwords_plus = []\n",
    "stopwords_plus = stopwords + lots_of_stopwords\n",
    "stopwords_plus = set(stopwords_plus)\n",
    "\n",
    "#Stopwords_plus contain total set of all stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d7b7a",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f34c5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compatibility', 'system', 'linear', 'constraint', 'set', 'natural', 'number', 'criterion', 'compatibility', 'system', 'linear', 'diophantine', 'equation', 'strict', 'inequations', 'nonstrict', 'inequations', 'upper', 'bound', 'component', 'minimal', 'set', 'solution', 'algorithm', 'construction', 'minimal', 'generating', 'set', 'solution', 'type', 'system', 'criterion', 'corresponding', 'algorithm', 'constructing', 'minimal', 'supporting', 'set', 'solution', 'solving', 'type', 'system', 'system', 'mixed', 'type']\n"
     ]
    }
   ],
   "source": [
    "processed_text = []\n",
    "for word in lemmatized_text:\n",
    "    if word not in stopwords_plus:\n",
    "        processed_text.append(word)\n",
    "print (processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c93208",
   "metadata": {},
   "source": [
    "## Vocabulary creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10245226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['solution', 'solving', 'mixed', 'bound', 'diophantine', 'equation', 'nonstrict', 'corresponding', 'constructing', 'upper', 'strict', 'number', 'natural', 'linear', 'generating', 'inequations', 'type', 'component', 'supporting', 'set', 'system', 'minimal', 'compatibility', 'algorithm', 'construction', 'constraint', 'criterion']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(set(processed_text))\n",
    "print (vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87282b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "vocab_len = len(vocabulary)\n",
    "\n",
    "weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "\n",
    "score = np.zeros((vocab_len),dtype=np.float32)\n",
    "window_size = 3 # define how many word are there in each keyword/keyphrase\n",
    "covered_coocurrences = []\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    score[i]=1\n",
    "    for j in range(0,vocab_len):\n",
    "        if j==i:\n",
    "            weighted_edge[i][j]=0\n",
    "        else:\n",
    "            for window_start in range(0,(len(processed_text)-window_size)):\n",
    "                \n",
    "                window_end = window_start+window_size\n",
    "                \n",
    "                window = processed_text[window_start:window_end]\n",
    "                \n",
    "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "                    \n",
    "                    index_of_i = window_start + window.index(vocabulary[i])\n",
    "                    index_of_j = window_start + window.index(vocabulary[j])\n",
    "                    \n",
    "                    # index_of_x is the absolute position of the xth term in the window \n",
    "                    # (counting from 0) \n",
    "                    # in the processed_text\n",
    "                      \n",
    "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
    "                        covered_coocurrences.append([index_of_i,index_of_j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92e08744",
   "metadata": {},
   "outputs": [],
   "source": [
    "inout = np.zeros((vocab_len),dtype=np.float32)\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    for j in range(0,vocab_len):\n",
    "        inout[i]+=weighted_edge[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "266a0e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converging at iteration 1....\n"
     ]
    }
   ],
   "source": [
    "MAX_ITERATIONS = 50\n",
    "d=0.85\n",
    "threshold = 0.0001 #convergence threshold\n",
    "\n",
    "for iter in range(0,MAX_ITERATIONS):\n",
    "    prev_score = np.copy(score)\n",
    "    \n",
    "    for i in range(0,vocab_len):\n",
    "        \n",
    "        summation = 0\n",
    "        for j in range(0,vocab_len):\n",
    "            if weighted_edge[i][j] != 0:\n",
    "                summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
    "                \n",
    "        score[i] = (1-d) + d*(summation)\n",
    "    \n",
    "    if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n",
    "        print(\"Converging at iteration \"+str(iter)+\"....\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fa52843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of solution: 0.15\n",
      "Score of solving: 0.15\n",
      "Score of mixed: 0.15\n",
      "Score of bound: 0.15\n",
      "Score of diophantine: 0.15\n",
      "Score of equation: 0.15\n",
      "Score of nonstrict: 0.15\n",
      "Score of corresponding: 0.15\n",
      "Score of constructing: 0.15\n",
      "Score of upper: 0.15\n",
      "Score of strict: 0.15\n",
      "Score of number: 0.15\n",
      "Score of natural: 0.15\n",
      "Score of linear: 0.15\n",
      "Score of generating: 0.15\n",
      "Score of inequations: 0.15\n",
      "Score of type: 0.15\n",
      "Score of component: 0.15\n",
      "Score of supporting: 0.15\n",
      "Score of set: 0.15\n",
      "Score of system: 0.15\n",
      "Score of minimal: 0.15\n",
      "Score of compatibility: 0.15\n",
      "Score of algorithm: 0.15\n",
      "Score of construction: 0.15\n",
      "Score of constraint: 0.15\n",
      "Score of criterion: 0.15\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,vocab_len):\n",
    "    print(\"Score of \"+vocabulary[i]+\": \"+str(score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e168ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned Phrases (Candidate Keyphrases): \n",
      "\n",
      "[['compatibility'], ['system'], ['linear', 'constraint'], ['set'], ['natural', 'number'], ['criterion'], ['compatibility'], ['system'], ['linear', 'diophantine', 'equation'], ['strict', 'inequations'], ['nonstrict', 'inequations'], ['upper', 'bound'], ['component'], ['minimal', 'set'], ['solution'], ['algorithm'], ['construction'], ['minimal', 'generating', 'set'], ['solution'], ['type'], ['system'], ['criterion'], ['corresponding', 'algorithm'], ['constructing'], ['minimal', 'supporting', 'set'], ['solution'], ['solving'], ['type'], ['system'], ['system'], ['mixed', 'type']]\n"
     ]
    }
   ],
   "source": [
    "phrases = []\n",
    "\n",
    "phrase = \" \"\n",
    "for word in lemmatized_text:\n",
    "    \n",
    "    if word in stopwords_plus:\n",
    "        if phrase!= \" \":\n",
    "            phrases.append(str(phrase).strip().split())\n",
    "        phrase = \" \"\n",
    "    elif word not in stopwords_plus:\n",
    "        phrase+=str(word)\n",
    "        phrase+=\" \"\n",
    "\n",
    "print(\"Partitioned Phrases (Candidate Keyphrases): \\n\")\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "507b1b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Phrases (Candidate Keyphrases): \n",
      "\n",
      "[['compatibility'], ['system'], ['linear', 'constraint'], ['set'], ['natural', 'number'], ['criterion'], ['linear', 'diophantine', 'equation'], ['strict', 'inequations'], ['nonstrict', 'inequations'], ['upper', 'bound'], ['component'], ['minimal', 'set'], ['solution'], ['algorithm'], ['construction'], ['minimal', 'generating', 'set'], ['type'], ['corresponding', 'algorithm'], ['constructing'], ['minimal', 'supporting', 'set'], ['solving'], ['mixed', 'type']]\n"
     ]
    }
   ],
   "source": [
    "unique_phrases = []\n",
    "\n",
    "for phrase in phrases:\n",
    "    if phrase not in unique_phrases:\n",
    "        unique_phrases.append(phrase)\n",
    "\n",
    "print(\"Unique Phrases (Candidate Keyphrases): \\n\")\n",
    "print(unique_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa7c3538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinned Unique Phrases (Candidate Keyphrases): \n",
      "\n",
      "[['compatibility'], ['system'], ['linear', 'constraint'], ['natural', 'number'], ['criterion'], ['linear', 'diophantine', 'equation'], ['strict', 'inequations'], ['nonstrict', 'inequations'], ['upper', 'bound'], ['component'], ['minimal', 'set'], ['solution'], ['construction'], ['minimal', 'generating', 'set'], ['corresponding', 'algorithm'], ['constructing'], ['minimal', 'supporting', 'set'], ['solving'], ['mixed', 'type']]\n"
     ]
    }
   ],
   "source": [
    "for word in vocabulary:\n",
    "    #print word\n",
    "    for phrase in unique_phrases:\n",
    "        if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n",
    "            #if len(phrase)>1 then the current phrase is multi-worded.\n",
    "            #if the word in vocabulary is present in unique_phrases as a single-word-phrase\n",
    "            # and at the same time present as a word within a multi-worded phrase,\n",
    "            # then I will remove the single-word-phrase from the list.\n",
    "            unique_phrases.remove([word])\n",
    "            \n",
    "print(\"Thinned Unique Phrases (Candidate Keyphrases): \\n\")\n",
    "print(unique_phrases)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af1321",
   "metadata": {},
   "source": [
    "## Scoring Vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88b9fcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: 'compatibility', Score: 0.15000000596046448\n",
      "Keyword: 'system', Score: 0.15000000596046448\n",
      "Keyword: 'linear constraint', Score: 0.30000001192092896\n",
      "Keyword: 'natural number', Score: 0.30000001192092896\n",
      "Keyword: 'criterion', Score: 0.15000000596046448\n",
      "Keyword: 'linear diophantine equation', Score: 0.45000001788139343\n",
      "Keyword: 'strict inequations', Score: 0.30000001192092896\n",
      "Keyword: 'nonstrict inequations', Score: 0.30000001192092896\n",
      "Keyword: 'upper bound', Score: 0.30000001192092896\n",
      "Keyword: 'component', Score: 0.15000000596046448\n",
      "Keyword: 'minimal set', Score: 0.30000001192092896\n",
      "Keyword: 'solution', Score: 0.15000000596046448\n",
      "Keyword: 'construction', Score: 0.15000000596046448\n",
      "Keyword: 'minimal generating set', Score: 0.45000001788139343\n",
      "Keyword: 'corresponding algorithm', Score: 0.30000001192092896\n",
      "Keyword: 'constructing', Score: 0.15000000596046448\n",
      "Keyword: 'minimal supporting set', Score: 0.45000001788139343\n",
      "Keyword: 'solving', Score: 0.15000000596046448\n",
      "Keyword: 'mixed type', Score: 0.30000001192092896\n"
     ]
    }
   ],
   "source": [
    "phrase_scores = []\n",
    "keywords = []\n",
    "for phrase in unique_phrases:\n",
    "    phrase_score=0\n",
    "    keyword = ''\n",
    "    for word in phrase:\n",
    "        keyword += str(word)\n",
    "        keyword += \" \"\n",
    "        phrase_score+=score[vocabulary.index(word)]\n",
    "    phrase_scores.append(phrase_score)\n",
    "    keywords.append(keyword.strip())\n",
    "\n",
    "i=0\n",
    "for keyword in keywords:\n",
    "    print (\"Keyword: '\"+str(keyword)+\"', Score: \"+str(phrase_scores[i]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be720109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyphrase:\n",
      "\n",
      "linear diophantine equation,  minimal supporting set,  minimal generating set,  mixed type,  corresponding algorithm,  linear constraint,  natural number,  minimal set,  upper bound,  nonstrict inequations,  "
     ]
    }
   ],
   "source": [
    "sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
    "\n",
    "keywords_num = 10\n",
    "\n",
    "print(\"Keyphrase:\\n\")\n",
    "\n",
    "for i in range(0,keywords_num):\n",
    "    print(str(keywords[sorted_index[i]])+\", \", end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ec97bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion,  natural,  solving,  mixed,  bound,  diophantine,  equation,  nonstrict,  corresponding,  constructing,  "
     ]
    }
   ],
   "source": [
    "sorted_index2 = np.flip(np.argsort(score),0)\n",
    "\n",
    "for i in range(0,keywords_num):\n",
    "    print(str(vocabulary[sorted_index2[i]])+\", \", end=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
