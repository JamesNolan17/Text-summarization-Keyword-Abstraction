{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/text-summarization-approaches-nlp-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89010ca4",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis\n",
    "https://medium.com/luisfredgs/automatic-text-summarization-with-machine-learning-an-overview-68ded5717a25 \\\n",
    "to identify semantically important sentences, not really a ML model? more like ranking \\\n",
    "github: https://github.com/luisfredgs/LSA-Text-Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0952941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e52ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemsCount(object):\n",
    "    def __init__(self, value):\n",
    "        self._value = value\n",
    "\n",
    "    def __call__(self, sequence):\n",
    "        if isinstance(self._value, (bytes, str,)):\n",
    "            if self._value.endswith(\"%\"):\n",
    "                total_count = len(sequence)\n",
    "                percentage = int(self._value[:-1])\n",
    "                # at least one sentence should be chosen\n",
    "                count = max(1, total_count*percentage // 100)\n",
    "                return sequence[:count]\n",
    "            else:\n",
    "                return sequence[:int(self._value)]\n",
    "        elif isinstance(self._value, (int, float)):\n",
    "            return sequence[:int(self._value)]\n",
    "        else:\n",
    "            ValueError(\"Unsuported value of items count '%s'.\" % self._value)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return to_string(\"<ItemsCount: %r>\" % self._value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ca0de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import attrgetter\n",
    "from collections import namedtuple\n",
    "\n",
    "SentenceInfo = namedtuple(\"SentenceInfo\", (\"sentence\", \"order\", \"rating\",))\n",
    "\n",
    "class BaseSummarizer(object):\n",
    "    \n",
    "    def __call__(self, document, sentences_count):\n",
    "        raise NotImplementedError(\"This method should be overriden in subclass\")\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_word(word):\n",
    "        return word.lower()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_best_sentences(sentences, count, rating, *args, **kwargs):\n",
    "        rate = rating\n",
    "        if isinstance(rating, dict):\n",
    "            assert not args and not kwargs\n",
    "            rate = lambda s: rating[s]\n",
    "\n",
    "        infos = (SentenceInfo(s, o, rate(s, *args, **kwargs))\n",
    "            for o, s in enumerate(sentences))\n",
    "\n",
    "        # sort sentences by rating in descending order\n",
    "        infos = sorted(infos, key=attrgetter(\"rating\"), reverse=True)\n",
    "        # get `count` first best rated sentences\n",
    "        if not isinstance(count, ItemsCount):\n",
    "            count = ItemsCount(count)\n",
    "        infos = count(infos)\n",
    "        # sort sentences by their order in document\n",
    "        infos = sorted(infos, key=attrgetter(\"order\"))\n",
    "\n",
    "        return tuple(i.sentence for i in infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0956fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import math\n",
    "import numpy\n",
    "import nltk\n",
    "\n",
    "from warnings import warn\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from numpy.linalg import svd as singular_value_decomposition\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class LsaSummarizer(BaseSummarizer):\n",
    "    MIN_DIMENSIONS = 3\n",
    "    REDUCTION_RATIO = 1/1\n",
    "\n",
    "    _stop_words = list(stopwords.words('english'))\n",
    "\n",
    "    @property\n",
    "    def stop_words(self):\n",
    "        return self._stop_words\n",
    "\n",
    "    @stop_words.setter\n",
    "    def stop_words(self, words):\n",
    "        self._stop_words = words\n",
    "\n",
    "    def __call__(self, document, sentences_count):\n",
    "\n",
    "        dictionary = self._create_dictionary(document)\n",
    "        \n",
    "        if not dictionary:\n",
    "            return ()\n",
    "\n",
    "        sentences = sent_tokenize(document)\n",
    "\n",
    "        matrix = self._create_matrix(document, dictionary)\n",
    "        matrix = self._compute_term_frequency(matrix)\n",
    "        u, sigma, v = singular_value_decomposition(matrix, full_matrices=False)\n",
    "\n",
    "        ranks = iter(self._compute_ranks(sigma, v))\n",
    "        return self._get_best_sentences(sentences, sentences_count,\n",
    "            lambda s: next(ranks))\n",
    "\n",
    "    def _create_dictionary(self, document):\n",
    "        \"\"\"Creates mapping key = word, value = row index\"\"\"\n",
    "\n",
    "        words = word_tokenize(document)\n",
    "        words = tuple(words)\n",
    "\n",
    "        words = map(self.normalize_word, words)\n",
    "\n",
    "        unique_words = frozenset(w for w in words if w not in self._stop_words)\n",
    "\n",
    "        return dict((w, i) for i, w in enumerate(unique_words))\n",
    "\n",
    "    def _create_matrix(self, document, dictionary):\n",
    "        \"\"\"\n",
    "        Creates matrix of shape where cells\n",
    "        contains number of occurences of words (rows) in senteces (cols).\n",
    "        \"\"\"\n",
    "        sentences = sent_tokenize(document)\n",
    "        words_count = len(dictionary)\n",
    "        sentences_count = len(sentences)\n",
    "        if words_count < sentences_count:\n",
    "            message = (\n",
    "                \"Number of words (%d) is lower than number of sentences (%d). \"\n",
    "                \"LSA algorithm may not work properly.\"\n",
    "            )\n",
    "            warn(message % (words_count, sentences_count))\n",
    "\n",
    "        matrix = numpy.zeros((words_count, sentences_count))\n",
    "        for col, sentence in enumerate(sentences):\n",
    "            words = word_tokenize(sentence)\n",
    "            for word in words:\n",
    "                # only valid words is counted (not stop-words, ...)\n",
    "                if word in dictionary:\n",
    "                    row = dictionary[word]\n",
    "                    matrix[row, col] += 1\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def _compute_term_frequency(self, matrix, smooth=0.4):\n",
    "        \"\"\"\n",
    "        Computes TF metrics for each sentence (column) in the given matrix and  normalize \n",
    "        the tf weights of all terms occurring in a document by the maximum tf in that document \n",
    "        according to ntf_{t,d} = a + (1-a)\\frac{tf_{t,d}}{tf_{max}(d)^{'}}.\n",
    "        \n",
    "        The smoothing term $a$ damps the contribution of the second term - which may be viewed \n",
    "        as a scaling down of tf by the largest tf value in $d$\n",
    "        \"\"\"\n",
    "        assert 0.0 <= smooth < 1.0\n",
    "\n",
    "        max_word_frequencies = numpy.max(matrix, axis=0)\n",
    "        rows, cols = matrix.shape\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                max_word_frequency = max_word_frequencies[col]\n",
    "                if max_word_frequency != 0:\n",
    "                    frequency = matrix[row, col]/max_word_frequency\n",
    "                    matrix[row, col] = smooth + (1.0 - smooth)*frequency\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def _compute_ranks(self, sigma, v_matrix):\n",
    "        assert len(sigma) == v_matrix.shape[0]\n",
    "\n",
    "        dimensions = max(LsaSummarizer.MIN_DIMENSIONS,\n",
    "            int(len(sigma)*LsaSummarizer.REDUCTION_RATIO))\n",
    "        powered_sigma = tuple(s**2 if i < dimensions else 0.0\n",
    "            for i, s in enumerate(sigma))\n",
    "\n",
    "        ranks = []\n",
    "        \n",
    "        for column_vector in v_matrix.T:\n",
    "            rank = sum(s*v**2 for s, v in zip(powered_sigma, column_vector))\n",
    "            ranks.append(math.sqrt(rank))\n",
    "\n",
    "        return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d6a0ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Original text =====\n",
      "De acordo com o especialista da Certsys (empresa que tem trabalhado na implementação e alteração de fluxos desses robôs), Diego Howës, as empresas têm buscado incrementar os bots de atendimento ao público interno com essas novas demandas de prevenção, para que os colaboradores possam ter à mão informações sobre a doença, tipos de cuidado, boas práticas de higiene e orientações gerais sobre a otimização do home office. Já os negócios que buscam se comunicar com o público externo enxergam outras necessidades. “Temos clientes de varejo que pediram para que fossem criados novos fluxos abordando o tema, e informando aos consumidores que as entregas dos produtos adquiridos online podem sofrer algum atraso”, comenta Howës, da Certsys, que tem buscado ampliar o escopo desses canais para se adequar ao momento de atenção. Ainda segundo o especialista, em todo o mercado é possível observar uma tendência de automatização do atendimento à população, em busca de chatbots que trabalhem em canais de alto acesso, como o WhatsApp, no caso de órgãos públicos. Na área de saúde, a disseminação de informação sobre a pandemia do vírus tem sido um esforço realizado.\n",
      "====== End of original text =====\n",
      "\n",
      "========= Summary =========\n",
      "De acordo com o especialista da Certsys (empresa que tem trabalhado na implementação e alteração de fluxos desses robôs), Diego Howës, as empresas têm buscado incrementar os bots de atendimento ao público interno com essas novas demandas de prevenção, para que os colaboradores possam ter à mão informações sobre a doença, tipos de cuidado, boas práticas de higiene e orientações gerais sobre a otimização do home office. Já os negócios que buscam se comunicar com o público externo enxergam outras necessidades. Na área de saúde, a disseminação de informação sobre a pandemia do vírus tem sido um esforço realizado.\n",
      "========= End of summary =========\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# source_file = \"original_text.txt\"\n",
    "\n",
    "# with open(source_file, \"r\", encoding='utf-8') as file:\n",
    "#     text = file.readlines()\n",
    "\n",
    "text = 'De acordo com o especialista da Certsys (empresa que tem trabalhado na implementação e alteração de fluxos desses robôs), Diego Howës, as empresas têm buscado incrementar os bots de atendimento ao público interno com essas novas demandas de prevenção, para que os colaboradores possam ter à mão informações sobre a doença, tipos de cuidado, boas práticas de higiene e orientações gerais sobre a otimização do home office. Já os negócios que buscam se comunicar com o público externo enxergam outras necessidades. “Temos clientes de varejo que pediram para que fossem criados novos fluxos abordando o tema, e informando aos consumidores que as entregas dos produtos adquiridos online podem sofrer algum atraso”, comenta Howës, da Certsys, que tem buscado ampliar o escopo desses canais para se adequar ao momento de atenção. Ainda segundo o especialista, em todo o mercado é possível observar uma tendência de automatização do atendimento à população, em busca de chatbots que trabalhem em canais de alto acesso, como o WhatsApp, no caso de órgãos públicos. Na área de saúde, a disseminação de informação sobre a pandemia do vírus tem sido um esforço realizado.'\n",
    "\n",
    "\n",
    "summarizer = LsaSummarizer()\n",
    "\n",
    "stopwords = stopwords.words('portuguese')\n",
    "summarizer.stop_words = stopwords\n",
    "summary =summarizer(text, 3)\n",
    "\n",
    "print(\"====== Original text =====\")\n",
    "print(text)\n",
    "print(\"====== End of original text =====\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n========= Summary =========\")\n",
    "\n",
    "print(\" \".join(summary))\n",
    "print(\"========= End of summary =========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a845b3a",
   "metadata": {},
   "source": [
    "## Encoder-Decoder network with Attention\n",
    "https://towardsdatascience.com/text-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e \\\n",
    "https://colab.research.google.com/gist/sayakmisra/6133be0554ce916d8cae4cdb83d475d8/text-summarization-seq2seq.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504d56f5",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/abstractive-summarization-using-pytorch-f5063e67510 \\\n",
    "https://towardsdatascience.com/text-summarization-using-deep-neural-networks-e7ee7521d804 \\\n",
    "https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd6fea355522f5d03011281e8b79f97380be3f5d3a4f423dda187c1a42b428a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
